# Text Classification with Multiple Embeddings - Group 6

## ğŸ“‹ Project Overview

Comparative analysis of text classification performance using different model architectures and word embedding techniques. This project implements and evaluates multiple embedding-model combinations for spam detection.

**Course:** Machine Learning Techniques I  
**Date:** February 2026  
**Dataset:** Spam Detection (SMS Spam Collection)

## ğŸ‘¥ Team Members

| Name                | Model               | Embeddings              | Contact           |
| ------------------- | ------------------- | ----------------------- | ----------------- |
| Mitali Bela         | Logistic Regression | TF-IDF, Skip-gram, CBOW | m.bela@alustudent.com |
| Charlotte Kariza    | RNN                 | TF-IDF, Skip-gram, CBOW | c.kariza@alustudent.com |
| Ntezirizaza Erneste | LSTM                | TF-IDF, Skip-gram, CBOW | e.nteziriza@alustudent.com |
| Orpheus Manga       | GRU                 | TF-IDF, Skip-gram, CBOW | o.manga@alustudent.com |

## ğŸ¯ Objectives

1. Implement and evaluate 4 different model architectures
2. Compare performance across multiple word embedding techniques:
   - TF-IDF (Term Frequency-Inverse Document Frequency)
   - Skip-gram (Word2Vec)
   - CBOW (Word2Vec)
3. Produce comprehensive comparative analysis with academic rigor
4. Document findings in research-style report with proper citations

## ğŸ“ Project Structure

```
Group6-Text-Classification-Assignment/
â”‚
â”œâ”€â”€ README.md                              # This file
â”œâ”€â”€ requirements.txt                        # Python dependencies
â”œâ”€â”€ .gitignore                             # Git ignore rules
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                               # Original dataset (don't modify)
â”‚   â”‚   â””â”€â”€ spam.csv
â”‚   â”œâ”€â”€ processed/                         # Preprocessed data
â”‚   â””â”€â”€ README.md                          # Dataset documentation
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb          # SHARED - EDA (4+ visualizations)
â”‚   â”œâ”€â”€ 02_member1_logistic_regression.ipynb
â”‚   â”œâ”€â”€ 03_member2_rnn.ipynb
â”‚   â”œâ”€â”€ 04_member3_lstm.ipynb
â”‚   â””â”€â”€ 05_member4_gru.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocessing.py                   # SHARED - Text cleaning
â”‚   â”œâ”€â”€ embeddings.py                      # SHARED - All embedding methods
â”‚   â”œâ”€â”€ utils.py                           # SHARED - Helper functions
â”‚   â””â”€â”€ evaluation.py                      # SHARED - Metrics & visualizations
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ figures/                           # All plots/visualizations
â”‚   â”œâ”€â”€ tables/                            # CSV result tables
â”‚   â””â”€â”€ comparison_results.csv             # Combined team results
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ BSE Group Assignments _ Task Sheet_Machine Learning Techniques I_C1_#_Group 6#].xlsx
```

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- pip or conda package manager
- Git

### Installation

1. **Clone the repository**

```bash
git clone https://github.com/ernesteNtezirizaza/Group6-Text-Classification-Assignment/
cd Group6-Text-Classification-Assignment
```

2. **Create virtual environment (recommended)**

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**

```bash
pip install -r requirements.txt
```

4. **Download required embedding models** (if using pre-trained)

```python
# Run in Python
import nltk
import gensim.downloader as api

nltk.download('punkt')
nltk.download('stopwords')

# Download GloVe (optional)
# glove_model = api.load('glove-wiki-gigaword-100')
```

## ğŸ’» Usage

### Step 1: Data Exploration (TEAM TASK)

```bash
jupyter notebook notebooks/01_data_exploration.ipynb
```

**Requirements:**

- 4+ visualizations (class balance, text length distribution, word clouds, vocabulary analysis)
- Statistical analysis
- Preprocessing strategy definition

### Step 2: Individual Model Development

Each member works on their assigned notebook:

```bash
jupyter notebook notebooks/02_member1_logistic_regression.ipynb
# OR
jupyter notebook notebooks/03_member2_rnn.ipynb
# etc.
```

**Each notebook must:**

- Implement the assigned model
- Train with at least 3 different embeddings
- Perform hyperparameter tuning
- Generate evaluation metrics (accuracy, F1, confusion matrix)
- Save results to `results/tables/`

## ğŸ“Š Evaluation Metrics

All models will be evaluated using:

- **Accuracy**: Overall classification accuracy
- **Precision**: Per-class precision
- **Recall**: Per-class recall
- **F1-Score**: Harmonic mean of precision and recall
- **Confusion Matrix**: Visual representation of predictions
- **Training Time**: Computational efficiency

## ğŸ”§ Shared Modules

### `src/preprocessing.py`

- Text cleaning (remove punctuation, lowercase, etc.)
- Tokenization
- Stop word removal
- Stemming/Lemmatization

### `src/embeddings.py`

- TF-IDF vectorization
- Word2Vec (Skip-gram & CBOW) training
- GloVe loading and processing
- FastText training
- Embedding adaptation for different models

### `src/utils.py`

- Data loading helpers
- Train/test split utilities
- Model saving/loading
- Logging utilities

## ğŸ“ Contribution Guidelines

### Version Control Workflow

1. **Pull latest changes** before starting work

   ```bash
   git pull origin main
   ```

2. **Create feature branch** for your work

   ```bash
   git checkout -b member1-logistic-regression
   ```

3. **Commit regularly** with clear messages

   ```bash
   git add .
   git commit -m "Add TF-IDF implementation for Logistic Regression"
   ```

4. **Push to remote**

   ```bash
   git push origin member1-logistic-regression
   ```

5. **Create Pull Request** for review

### Code Style

- Follow PEP 8 for Python code
- Add docstrings to all functions
- Comment complex logic
- Use meaningful variable names

### Documentation Requirements

- Update `docs/BSE Group Assignments _ Task Sheet_Machine Learning Techniques I_C1_#_Group 6#].xlsx` after each work session
- Document all experiments in notebooks
- Add citations for techniques used
- Keep README updated

## ğŸ“š Key References

### Word Embeddings

- Mikolov et al. (2013). "Efficient Estimation of Word Representations in Vector Space" (Word2Vec)
- Pennington et al. (2014). "GloVe: Global Vectors for Word Representation"
- Bojanowski et al. (2017). "Enriching Word Vectors with Subword Information" (FastText)

## ğŸ“‹ Deliverables Checklist

- [ ] **GitHub Repository**
  - [ ] Clean, well-documented code
  - [ ] Meaningful README
  - [ ] All notebooks functional
  - [ ] Proper .gitignore

- [ ] **PDF Report** (Academic Format)
  - [ ] Introduction & problem statement
  - [ ] Literature review with citations
  - [ ] Methodology (dataset, preprocessing, models, embeddings)
  - [ ] Results (2+ comparison tables, visualizations)
  - [ ] Discussion (analysis, limitations, insights)
  - [ ] Conclusion & future work
  - [ ] References (APA/IEEE format)
  - [ ] Contribution tracker included
  - [ ] Link to GitHub repo

- [ ] **Experiments**
  - [ ] Each member: 1 model Ã— 3+ embeddings
  - [ ] Hyperparameter tuning documented
  - [ ] All results in `results/` folder

## ğŸ¤ Communication

- **Team Meetings:** Google meet
- **Communication Channel:** WhatsApp
- **Sharing Documents** 

## ğŸ“ Contact

For questions or issues, contact:

- Team Lead: Ntezirizaza Erneste - e.nteziriza@alustudent.com
- Course Instructor: Samiratu - sntohsi@alueducation.com

---

**Repository:** [\[GitHub URL\]](https://github.com/ernesteNtezirizaza/Group6-Text-Classification-Assignment)  
